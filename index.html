<!DOCTYPE html>
<!-- saved from url=(0031)https://ayush.sekhari.com/#home -->
<html lang="en" style="" class=" js no-touch cssanimations csstransforms csstransforms3d csstransitions"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="" src="./Vaidehi_Patil_files/"></script>
    <!-- <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-111452540-1');
    </script> -->

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge"> 
    <title>Vaidehi Patil</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Vaidehi is a PhD student in Computer Science at UNC Chapel Hill.">
    <meta name="keywords" content="unlearning, CV, safety, security, privacy, AI, NLP">
    <meta name="author" content="lmtheme">
    <link rel="shortcut icon" href="https://https://vaidehipatil16.github.io/favicon.ico">
    <meta property="og:description" content="Vaidehi is a PhD student in Computer Science at UNC Chapel Hill.">
    <meta property="og:title" content="Vaidehi Patil">
    <meta property="og:type" content="website">

    <link rel="stylesheet" href="./Vaidehi_Patil_files/" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/normalize.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/animate.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/transition-animations.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/jquery.mCustomScrollbar.min.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/owl.carousel.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/magnific-popup.css" type="text/css">
    <link rel="stylesheet" href="./Vaidehi_Patil_files/main-red.css" type="text/css">

    <!-- This styles needs for demo -->
    <!-- <link rel="stylesheet" href="preview/lmpixels-demo-panel.css" type="text/css"> -->
    <!-- /This styles needs for demo -->


    <script src="./Vaidehi_Patil_files/jquery-2.1.3.min.js"></script>
    <script src="./Vaidehi_Patil_files/modernizr.custom.js"></script>
    <script src="./Vaidehi_Patil_files/aqi.js"></script>
    <script src="./Vaidehi_Patil_files/snack.js"></script>
  </head>
  <body onload="snackload();" data-new-gr-c-s-check-loaded="14.1229.0" data-gr-ext-installed="">

  
    <div id="snackbar" class="">This website is best viewed on a desktop browser.</div>
    <!-- Loading animation -->
    <div class="preloader" style="display: none;">
      <div class="preloader-animation">
        <div class="dot1"></div>
        <div class="dot2"></div>
      </div>
    </div>
    <!-- /Loading animation -->

    <div id="page" class="page">
      <!-- Header -->
      <header id="site_header" class="header mobile-menu-hide header-color-light mCustomScrollbar _mCS_8 mCS_no_scrollbar"><div id="mCSB_8" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_8_container" class="mCSB_container mCS_y_hidden mCS_no_scrollbar_y" style="position:relative; top:0; left:0;" dir="ltr">
       <!--  <div class="my-photo tilt-effect">
          <img src="images/photo.png" alt="image">
        </div> -->

        <div class="site-title-block">
          <h1 class="site-title">Vaidehi Patil</h1>
        </div>

        <!-- Navigation -->
        <div class="site-nav">
          <!-- Main menu -->
          <ul id="nav" class="site-main-menu">
            <li class="active">
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#home" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-id"></i>Home</a>
            </li>
            <li>
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#Publications" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-news-paper"></i>Publications</a>
            </li>
            <li>
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#InvitedTalks" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-video"></i>Invited Talks</a>
            </li>
            <li>
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#Teaching" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-study"></i>Teaching</a>
            </li>
            <li>
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#Service" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-portfolio"></i>Service</a>
            </li>
            <li>
              <a class="pt-trigger" href="https://vaidehipatil16.github.io/#Misc" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-timer"></i>Misc</a>
            </li>
            <!-- <li>
              <a class="pt-trigger" href="#contact" data-animation="19"><i class="menu-icon pe-7s-icon pe-7s-mail"></i>Contact</a>
            </li> -->
          </ul>
          <!-- /Main menu --> 
        </div>
        <!-- Navigation -->
      </div><div id="mCSB_8_scrollbar_vertical" class="mCSB_scrollTools mCSB_8_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: none;"><div class="mCSB_draggerContainer"><div id="mCSB_8_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; height: 0px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></header>
      <!-- /Header -->

      <!-- Mobile Header -->
      <div class="mobile-header mobile-visible">
        <div class="mobile-logo-container">
          <div class="mobile-site-title">Vaidehi Patil</div>
        </div>

        <a class="menu-toggle mobile-visible">
          <i class="fa fa-bars"></i>
        </a>
      </div>
      <!-- /Mobile Header -->

      <!-- Main Content -->
      <div id="main" class="site-main">
        <!-- Page changer wrapper -->
        <div class="pt-wrapper" style="background-image: url(images/bg.png);">
          <!-- Subpages -->
          <div class="subpages">

            <!-- About Me Subpage -->
            <section class="pt-page pt-page-4 pt-page-current" data-id="home"><div id="mCSB_1" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_1_container" class="mCSB_container" style="position: relative; top: -1016px; left: 0px;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">


                <div class="row" style="background-color: #FBF4F3; padding: 20px; padding-right: 35px; margin-top: 10px;">
                  <div class="col-12 col-sm-6 col-md-4 subpage-block" style="display: flex; align-items: center; justify-content: center; flex-direction: column; margin-bottom: -20px;">
                    <div style="align-items: center; justify-content: center;">
                      <div class="custom-image-container">
                        <img src="./Vaidehi_Patil_files/avatar.JPG" alt="" class="mCS_img_loaded">
                      </div>
                      <p align="center">
                        <a href="https://vaidehipatil16.github.io/docs/CV_Vaidehi_Patil.pdf" target="_blank">CV</a> |
                        <!-- <a href="mailto:sekhari@mit.edu">Email</a> | -->
                        <a href="https://scholar.google.com/citations?user=wCt6wSAAAAAJ&hl=en" target="_blank">Google Scholar</a><br>
                        <a href="mailto:vaidehi@cs.unc.edu"> Email: <u>vaidehi@cs.unc.edu</u></a>
                      </p>
                      <br>
                    </div> 
                  </div>
                
                  <div class="justify" ,="" style="margin-top: 10px;">
                    <!-- <br> -->
                    Hi, this is <strong> Vaidehi Patil</strong>. I am a PhD student at UNC Chapel Hill, where I am being advised by 
                    <a href="https://www.cs.unc.edu/~mbansal/" target="_blank">Prof. Mohit Bansal</a>. 
                    <strong>My research goal is to make deep learning models more safe and responsible for real world applications. For this goal, I work on topics like safety, privacy, security for LLMs, multimodal models, multi-agent systems. Iâ€™m also interested in controlling and steering ML models using techniques such as model editing.
                    <br><br>
                
                    I received my Interdisciplinary Dual Degree: B. Tech. in Electrical Engineering and M. Tech. in AI and Data Science and a Minor Degree in Computer Science and Engineering from IIT Bombay in 2022. At IIT Bombay, I was advised by 
                    <a href="https://www.cse.iitb.ac.in/~sunita/" target="_blank"> Prof. Sunita Sarawagi </a> from IIT Bombay and 
                    <a href="https://parthatalukdar.github.io/" target="_blank"> Partha Talukdar</a> from Google Research India. I have also spent wonderful summers as an intern at Adobe Research in 2020, 2021, at Amazon AGI in 2023 and Apple in 2024.
                    <br>

              
                  </div>
                </div>
                

              <br> 
              <h3 class="special-section-title">Research Highlights and Selected Papers</h3>

              
              <div class="justify">

              <span style="font-size: 20px;"><u><strong>Machine Unlearning</strong></u></span>: I have developed foundational techniques and efficient methods for securely removing data from ML models post-deployment, ensuring privacy and compliance with evolving data regulations and operational needs. 

              <ul> 
                <li> <p> <a href="https://arxiv.org/abs/2103.03279" target="_blank">Remember What You Want to Forget: Algorithms for Machine Unlearning</a>, <br> with Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh 
                  (NeurIPS 2021).
                  </p>
                </li>
              
                <li> <p> <a href="https://arxiv.org/abs/2306.15744" target="_blank">Ticketed Learning-Unlearning Schemes</a>, <br> with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang
                  (COLT 2023). 
                  </p>
                  </li>
              </ul>
              
              <span style="font-size: 20px;"><u><strong>Interactive Learning</strong></u></span>: I have created theoretically rigorous and practical algorithms for learning in dynamic, real-world environments. This includes new algorithmic frameworks for Reinforcement Learning (RL) and imitation learning. <!-- enabling interactive ML systems to adapt and function effectively under changing conditions.-->
              
              <ul>
                <li> <p> <a href="https://arxiv.org/abs/2307.04998" target="_blank">Selective Sampling and Imitation Learning via Online Regression</a>, <br> with Karthik Sridharan, Wen Sun, and Runzhe Wu (NeurIPS 2023). 
                </p>
                </li>
              
                <li> <p> <a href="https://arxiv.org/abs/2206.13063" target="_blank">On the Complexity of Adversarial Decision Making</a>, <br> with Dylan J. Foster, Alexander Rakhlin and Karthik Sridharan. (NeurIPS 2022, <font color="red"> oral presentation</font>). 
                  </p>
                </li> 
              
                <li> <p> <a href="https://arxiv.org/abs/2210.06718" target="_blank">Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</a>, 
                   <br> with Yuda Song, Yifei Zhou, J. Andrew Bagnell, Akshay Krishnamurthy, Wen Sun (ICLR 2023).  
                  </p>
                  </li>
              </ul>
              
              <span style="font-size: 20px;"><u><strong>Stochastic Optimization</strong></u></span>: I have advanced our understanding of first-order methods and developed new optimal algorithms for learning in large-scale, complex, non-convex models that are central to modern ML.
              
              <ul>
              <li> <p> <a href="https://arxiv.org/abs/2107.05074" target="_blank">SGD: The role of Implicit Regularization, Batch-size and Multiple Epochs</a>, <br> with Satyen Kale and Karthik Sridharan (NeurIPS 2021). 
                </p>
                </li> 
              
                <li> <p> <a href="https://arxiv.org/abs/2210.06705" target="_blank">From Gradient Flow on Population Loss to Learning with Stochastic Gradient Descent</a>, <br> with Satyen Kale, Jason D. Lee, Chris De Sa and Karthik Sridharan (NeurIPS 2022). 
                  </p>
                </li>
              
                <li> <p> <a href="https://arxiv.org/abs/1902.04686" target="_blank">The Complexity of Making the Gradient Small in Stochastic Convex Optimization</a>, <br> with Dylan Foster, Ohad Shamir, Nathan Srebro, Karthik Sridharan and Blake Woodworth (COLT 2019,  <font color="red">Best Student Paper Award</font>). 
                  </p>
                  </li>
              </ul>
              
              <br>

              <hr style="border: 0; height: 1px; margin-top: 0px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), #86311f, rgba(0, 0, 0, 0));">

              <h3 class="special-section-title">Recent News</h3>
              <ul style="margin-top: 0;">
                <li>[Jan'24] <a href="https://arxiv.org/abs/2501.13941" target="_blank">New paper</a> on arXiv on practical watermarking of Large Language Models (LLMs).</li>  
                <li>[Jan'24] Our paper on <a href="https://arxiv.org/abs/2406.11810">deterministic linear bellman complete MDPs</a> accepted at ICLR 2025 as Oral Presentation.</li> 
                <li>[Jan'24] Our paper on <a href="https://arxiv.org/abs/2406.17216">evaluating machine unlearning algorithms</a> accepted at ICLR 2025.</li> 
                <li>[Dec'24] I am attending NeurIPS 2024. Let us meet if you are around?</li> 
                <li>[Dec'24] Giving a short talk at the  <a href="https://pli.princeton.edu/events">Princeton Language and Intelligence seminar</a> on "Machine Unlearning: Where does the practice stand?".</li> 
                <li>[Dec'24] Giving a talk at Rutgers University on learning-unlearning schemes.</li> 
                <li> I will be an area chair for <a href="http://algorithmiclearningtheory.org/alt2025/" target="_blank"> Algorithmic Learning Theory (ALT) 2025</a>.</li>
              </ul> 
              
              <!-- Toggle Button -->
              <div style="text-align: center;">
                <button id="toggle-old-news" onclick="toggleOldNews()">Show Old News</button>
            </div>
                          
              <!-- Old News Items -->
              <ul id="old-news" style="display: none;">
                <li>Yunbei Xu and I are organizing a session on <b>Theoretical Aspects of Interactive Learning</b> under Applied Probability Society at INFORMS 2024 from Oct 20-23. Our awesome set of speakers include Noah Golowich, Zeyu Jia, Kevin Jamieson, and Yunzhong Xu.  </li>
                <li> I will be attending <a href="https://www.orie.cornell.edu/orie-events/young-researchers-workshop-2024" target="_blank">Cornell's ORIE Young Researchers Workshop</a> in Ithaca, NY from Oct 9-11. Let us chat if you are around! </li>
                <li> I will be attending <a href="https://www.simonsfoundation.org/event/mathematical-and-scientific-foundations-of-deep-learning-annual-meeting-2024/" target="_blank">Mathematical and Scientific Foundations of Deep Learning Annual Meeting</a> at Simons Institute, NYC from Sept 26-27. Let us chat if you are around! </li>
                <li>Two papers accepted at ICLR 2024. Looking forward to the conference in Vienna.</li> 
              </ul>
              
              <script>
                function toggleOldNews() {
                  var oldNews = document.getElementById("old-news");
                  var toggleButton = document.getElementById("toggle-old-news");
                  
                  if (oldNews.style.display === "none") {
                    oldNews.style.display = "block";
                    toggleButton.innerHTML = "Hide Old News";
                  } else {
                    oldNews.style.display = "none";
                    toggleButton.innerHTML = "Show Old News";
                  }
                }
              </script>
              
              <br>
              <br>
              </div>
              
              <!-- <hr style="border: 0; height: 1px; margin-top: 0px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), #86311f, rgba(0, 0, 0, 0));">
              <h3 class="special-section-title">Work Experience</h3> -->


              <hr style="border: 0; height: 1px; margin-top: 0px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), #86311f, rgba(0, 0, 0, 0));">
              <h3 class="special-section-title">Selected Awards</h3>
              <ul style="margin-top: 0;">
                <li>Best Student Paper Award at COLT 2019</li>
                <li>Finalist for Meta AI PhD Fellowship, 2022</li> 
                <li>Best Talk Award, Honorable Mention, New York Academy of Sciences, 2020</li> 
                <li>President's Gold Medal, IIT Kanpur, 2016</li>
              </ul> 

            </div></div><div id="mCSB_1_scrollbar_vertical" class="mCSB_scrollTools mCSB_1_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: block; visibility: hidden;"><div class="mCSB_draggerContainer"><div id="mCSB_1_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; display: block; height: 406px; max-height: 867px; top: 471px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

            <!-- Publications subpage -->
            <!-- Grad School Subpage -->
            <section class="pt-page pt-page-4 mCustomScrollbar _mCS_2" data-id="Publications"><div id="mCSB_2" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_2_container" class="mCSB_container" style="position:relative; top:0; left:0;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Publications and Preprints</h2>
                    <!-- <h5 class="section-description">Advise?</h5> -->
                  </div>
                </div>
                  
<!--               <div class="block-title">
                <h2>Publications</h2>
              </div>
 -->
<!--               <p> 
                In keeping with the convention in mathematical sciences, authors are listed alphabetically by their last name in papers denoted with <span style="color: red;">(Î±-Î²)</span> above my name.
              </p> -->

              <div style="display: flex; justify-content: center; align-items: center;">
              <div style="width: 92%; padding-top: 7px; padding-left: 8px; padding-right: 8px; background-color: #fffae5; border: 0px solid #F7FF00; border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); text-align: center;">
                <p>
                    In keeping with the convention in mathematical sciences, most of my papers list authors alphabetically by their last names. 
                    These papers are marked with the symbol <span style="color: black;">"@"</span> above my name. 
                     Otherwise, <span>*</span> denotes equal contributions. 
                </p>
              </div>
            </div>

            <!-- <sup style="color: red;">(Î±-Î²)</sup> -->
            <br>


              <div id="publications-container"><h2> Preprints/Working Papers </h2>
<ul> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2501.13941" target="_blank">GaussMark: A Practical Approach for Structural Watermarking of Language Models</a></strong>  &nbsp; <a class="text-button3">Arxiv</a>
          <br> Adam Block, Alexander Rakhlin, and Ayush Sekhari<sup>@</sup>  
          <br>               <a href="javascript:toggleblock(&#39;dash_abs30&#39;)">Abstract</a> |
          <a href="https://arxiv.org/abs/2501.13941">ArXiv</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->
  
          </p><p align="justify"> <i style="display: none;" id="dash_abs30"> Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.
         </i></p>  
      <p></p>
  </li> 

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2410.08074" target="_blank">Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</a> </strong> &nbsp; <a class="text-button3">Arxiv</a>
          <br> Vinith M. Suriyakumar*, Rohan Alur*, Ayush Sekhari, Manish Raghavan, Ashia C. Wilson
          <br>               <a href="javascript:toggleblock(&#39;dash_abs28&#39;)">Abstract</a> |
          <a href="https://arxiv.org/abs/2410.08074">ArXiv</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->
  
          </p><p align="justify"> <i style="display: none;" id="dash_abs28">Text-to-image diffusion models rely on massive, web-scale datasets. Training them from scratch is computationally expensive, and as a result, developers often prefer to make incremental updates to existing models. These updates often compose fine-tuning steps (to learn new concepts or improve model performance) with "unlearning" steps (to "forget" existing concepts, such as copyrighted works or explicit content). In this work, we demonstrate a critical and previously unknown vulnerability that arises in this paradigm: even under benign, non-adversarial conditions, fine-tuning a text-to-image diffusion model on seemingly unrelated images can cause it to "relearn" concepts that were previously "unlearned." We comprehensively investigate the causes and scope of this phenomenon, which we term concept resurgence, by performing a series of experiments which compose "mass concept erasure" (the current state of the art for unlearning in text-to-image diffusion models (Lu et al., 2024)) with subsequent fine-tuning of Stable Diffusion v1.4. Our findings underscore the fragility of composing incremental model updates, and raise serious new concerns about current approaches to ensuring the safety and alignment of text-to-image diffusion models.
         </i></p>  

          <!-- <br> Preprint arXiv:2410.08074.--> 
      <p></p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2407.04264" target="_blank">Langevin Dynamics: A Unified Perspective on Optimization via Lyapunov Potentials</a> </strong> &nbsp; <a class="text-button3">Arxiv</a>
          <br> August Y. Chen, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan   
          <br> Preliminary version at OPT for ML 2024 Workshop at NeurIPS 2024.
          <br>               <a href="javascript:toggleblock(&#39;dash_abs27&#39;)">Abstract</a> |
          <a href="https://arxiv.org/abs/2407.04264">ArXiv</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->
  
          </p><p align="justify"> <i style="display: none;" id="dash_abs27">We study the problem of non-convex optimization using Stochastic Gradient Langevin Dynamics (SGLD). SGLD is a natural and popular variation of stochastic gradient descent where at each step, appropriately scaled Gaussian noise is added. To our knowledge, the only strategy for showing global convergence of SGLD on the loss function is to show that SGLD can sample from a stationary distribution which assigns larger mass when the function is small (the Gibbs measure), and then to convert these guarantees to optimization results.
            We employ a new strategy to analyze the convergence of SGLD to global minima, based on Lyapunov potentials and optimization. We convert the same mild conditions from previous works on SGLD into geometric properties based on Lyapunov potentials. This adapts well to the case with a stochastic gradient oracle, which is natural for machine learning applications where one wants to minimize population loss but only has access to stochastic gradients via minibatch training samples. Here we provide 1) improved rates in the setting of previous works studying SGLD for optimization, 2) the first finite gradient complexity guarantee for SGLD where the function is Lipschitz and the Gibbs measure defined by the function satisfies a PoincarÃ© Inequality, and 3) prove if continuous-time Langevin Dynamics succeeds for optimization, then discrete-time SGLD succeeds under mild regularity assumptions. 
         </i></p>  
      <p></p>
  </li> 

  <li> 
      <p> 
          <strong>The Role of Environment Access in Agnostic Reinforcement Learning</strong>  <!-- &nbsp; <a class="text-button3">Arxiv</a>--> 
          <br> Akshay Krishnamurthy, Gene Li, and Ayush Sekhari<sup>@</sup>  
          <br>               <a href="javascript:toggleblock(&#39;dash_abs29&#39;)">Abstract</a> |
          <a href="https://ayush.sekhari.com/">In Submission</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> --> 
  
          </p><p align="justify"> <i style="display: none;" id="dash_abs29"> We study Reinforcement Learning (RL) in environments with large state spaces, where function approximation is required for sample-efficient learning. Departing from a long history of prior work, we consider the weakest possible form of function approximation, called agnostic policy learning, where the learner seeks to find the best policy in a given class $\Pi$, with no guarantee that $\Pi$ contains an optimal policy for the underlying task. Although it is known that sample-efficient agnostic policy learning is not possible in the standard online RL setting without further assumptions, we investigate the extent to which this can be overcome with stronger forms of access to the environment. Specifically, we show that:

-Agnostic policy learning remains statistically intractable when given access to a local simulator, from which one can reset to any previously seen state. This result holds even when the policy class is realizable, and stands in contrast to a positive result of [MFR24] showing that value-based learning under realizability is tractable with local simulator access.

-Agnostic policy learning remains statistically intractable when given online access to a reset distribution with good coverage properties over the state space (the so-called $\mu$-reset setting). We also study stronger forms of function approximation for policy learning, showing that PSDP [BKSN03] and CPI [KL02] provably fail in the absence of policy completeness.

-On a positive note, agnostic policy learning is statistically tractable for Block MDPs with access to both of the above reset models. We establish this via a new algorithm that carefully constructs a policy emulator: a tabular MDP with a small state space that approximates the value functions of all $\pi \in \Pi$. These values are approximated without any explicit value function class.
Taken together, our results contribute to a deeper understanding of the interplay between function approximation and environment access in RL. 
         </i></p>  
      <p></p>
  </li> 


    <li> 
        <p> 
            <strong> The Space Complexity of Learning-Unlearning Schemes </strong> 
            <br> Yeshwanth Cherapanamjeri, Sumegha Garg, Nived Rajaraman, Ayush Sekhari<sup style="color: black;">@</sup>, and Abhishek Shetty 
        </p>
    </li> 

    <li> 
        <p> 
            <strong> System Aware Unlearning Algorithms: Use Lesser, Forget Faster </strong> 
            <br> Linda Lu, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
            <br>  
        </p>
    </li> 

</ul> 


<h2> Conference Publications </h2>
<ul> 


    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2406.17216" target="_blank">Machine Unlearning Fails to Remove Data Poisoning Attacks</a> </strong> &nbsp; <a class="text-button3">ICLR 2025</a>
            <br> Martin Pawelczyk*, Jimmy Z. Di*, Yiwei Lu, Gautam Kamath*, Ayush Sekhari* (equal advisory contribution), and Seth Neel* 
            <br> Preliminary version accepted as <font color="red">spotlight presentation</font> at Generative AI and Law Workshop (GenLaw'24) at ICML 2024.
            <br>               <a href="javascript:toggleblock(&#39;dash_abs26&#39;)">Abstract</a> |
            <a href="https://arxiv.org/abs/2406.17216">ArXiv</a> 
            <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

            </p><p align="justify"> <i style="display: none;" id="dash_abs26">We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of training on poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of evaluation settings (e.g., alleviating membership inference attacks), they fail to remove the effects of data poisoning, across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, is required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned datapoints without having to retrain, our work suggests that these methods are not yet "ready for prime time", and currently provide limited benefit over retraining.
           </i></p>  
        <p></p>
    </li>

    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2406.11810" target="_blank">Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</a> </strong> &nbsp; <a class="text-button3">ICLR 2025</a> <a class="text-button">Oral</a> 
            <br> Runzhe Wu*, Ayush Sekhari* (equal contribution), Akshay Krishnamurthy, Wen Sun 
            <br>               <a href="javascript:toggleblock(&#39;dash_abs25&#39;)">Abstract</a> |
            <a href="https://arxiv.org/abs/2406.11810">ArXiv</a>  | 
            <a href="https://www.youtube.com/watch?v=TFdHS-o5ss8&amp;ab_channel=RLtheoryseminars">Talk (by Runzhe Wu)</a> 
            <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

            </p><p align="justify"> <i style="display: none;" id="dash_abs25">We study computationally and statistically efficient Reinforcement Learning algorithms for the linear Bellman Complete setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.
           </i></p>  

        <p></p>
    </li>

    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2403.17091" target="_blank">Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data</a> </strong>  
          
            &nbsp; <a class="text-button3">COLT 2024</a>
            <br> Zeyu Jia, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Chen-Yu Wei  

            <br> 
              <a href="javascript:toggleblock(&#39;dash_abs24&#39;)">Abstract</a> |
              <a href="https://arxiv.org/abs/2403.17091">ArXiv</a>  | 
              <a href="https://www.youtube.com/watch?v=kQ9v0sy8Cao&amp;ab_channel=RLtheoryseminars">Talk (by Zeyu Jia)</a> 
              <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

              </p><p align="justify"> <i style="display: none;" id="dash_abs24">We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the offline data is admissible (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data. These three pieces jointly resolve the open problem, though each of them could be of independent interest.
            </i></p>                      
        <p></p> 
    </li> 



  <li> 
    <p> 
        <strong> <a href="https://srinathm1359.github.io/random-latent-exploration/" target="_blank">Random Latent Exploration for Deep Reinforcement Learning</a> </strong>  &nbsp; <a class="text-button3">ICML 2024</a> 
        <br> Srinath Mahankali, Zhang-Wei Hong, Ayush Sekhari, Alexander Rakhlin, Pulkit Agrawal
        <br> 
        <a href="javascript:toggleblock(&#39;dash_abs23&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2407.13755">ArXiv</a>  | 
        <a href="https://srinathm1359.github.io/random-latent-exploration/"> Project Website </a> | 
        <a class="fa fa-github w3-hover-opacity" href="https://github.com/Improbable-AI/random-latent-exploration"></a> 

        </p><p align="justify"> <i style="display: none;" id="dash_abs23">The ability to efficiently explore high-dimensional state spaces is essential for the practical success of deep Reinforcement Learning (RL). This paper introduces a new exploration technique called Random Latent Exploration (RLE), that combines the strengths of bonus-based and noise-based (two popular approaches for effective exploration in deep RL) exploration strategies. RLE leverages the idea of perturbing rewards by adding structured random rewards to the original task rewards in certain (random) states of the environment, to encourage the agent to explore the environment during training. RLE is straightforward to implement and performs well in practice. To demonstrate the practical effectiveness of RLE, we evaluate it on the challenging Atari and IsaacGym benchmarks and show that RLE exhibits higher overall scores across all the tasks than other approaches.
      </i></p>  
    <p></p>
</li> 



<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2401.09681" target="_blank">Harnessing Density Ratios for Online Reinforcement Learning</a> </strong> &nbsp; <a class="text-button3">ICLR 2024</a>           <a class="text-button">Spotlight</a>

        <br> Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari<sup style="color: black;">@</sup>, and Tengyang Xie  
        <br>               <a href="javascript:toggleblock(&#39;dash_abs22&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2401.09681">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs22">The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.
       </i></p>  
    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2311.08384" target="_blank">Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees</a> </strong> &nbsp; <a class="text-button3">ICLR 2024</a>
        <br> Yifei Zhou*, Ayush Sekhari* (equal contribution), Yuda Song, and Wen Sun
        <br>               <a href="javascript:toggleblock(&#39;dash_abs21&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2311.08384">ArXiv</a> | 
        <a href="https://ayush.sekhari.com/" https:="" github.com="" yifeizhou02="" hnpg"=""><i class="fa fa-github w3-hover-opacity"></i></a> | 
        <a href="https://youtu.be/WmO8Lp4DfmU">Talk (1 hr)</a>
    
        </p><p align="justify"> <i style="display: none;" id="dash_abs21">Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning. On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice. Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework. We show that our approach, in theory, can obtain a best-of-both-worlds type of result -- it achieves the state-of-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while at the same time maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity. Experimentally, in challenging rich-observation environments, we show that our approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization, demonstrating the empirical benefit of combining on-policy and off-policy learning. Our code is publicly available. 
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/pdf/2310.06113.pdf" target="_blank">When is Agnostic Reinforcement Learning Statistically Tractable?</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Nathan Srebro 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs20&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2310.06113">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs20">We study the problem of agnostic PAC reinforcement learning (RL): given a policy class Î , how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an Ïµ-suboptimal policy with respect to Î ? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set Î  and is independent of the MDP dynamics. With a generative model, we show that for any policy class Î , bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class Î  with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunflower} structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2307.04998" target="_blank">Selective Sampling and Imitation Learning via Online Regression</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Ayush Sekhari<sup style="color: black;">@</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
        <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a> at ICML 2023.  
        <br>               <a href="javascript:toggleblock(&#39;dash_abs19&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2307.04998">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs19">We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.
            Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t.~the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2307.12926" target="_blank">Contextual Bandits and Imitation Learning via Preference-Based Active Queries</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Ayush Sekhari<sup style="color: black;">@</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
        <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a>, and <a href="https://sites.google.com/view/mfpl-icml-2023" target="_blank">The Many Facets of Preference-Based Learning workshop</a> at ICML 2023.  

        <br>               <a href="javascript:toggleblock(&#39;dash_abs18&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2307.12926">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs18">We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as O(min{Tâ€¾â€¾âˆš,d/Î”}), where T represents the number of interactions, d represents the eluder dimension of the function class, and Î” represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of Î”, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only O(min{T,d2/Î”2}) queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length H each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2212.10717" target="_blank">Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Jimmy Z. Di*, Jack Douglas*, Jayadev Acharya, Gautam Kamath, and Ayush Sekhari 

        <br>               <a href="javascript:toggleblock(&#39;dash_abs17&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2212.10717">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs17">We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2211.14250" target="_blank">Model-Free Reinforcement Learning with the Decision-Estimation Coefficient</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Dylan J. Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari<sup style="color: black;">@</sup> 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs16&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2211.14250">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs16">We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give structural results showing when it can and cannot help more generally.
       </i></p>  
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2306.15744" target="_blank">Ticketed Learning-Unlearning Schemes</a> </strong> &nbsp; <a class="text-button3">COLT 2023</a>
        <br> Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Ayush Sekhari<sup style="color: black;">@</sup>, and Chiyuan Zhang
        <br> Short version at Symposium on the Foundations of Responsible Computing, FORC 2023.  
        <br>               <a href="javascript:toggleblock(&#39;dash_abs15&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2306.15744">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs15">We consider the learning--unlearning paradigm defined as follows. First given a dataset, the goal is to learn a good predictor, such as one minimizing a certain loss. Subsequently, given any subset of examples that wish to be unlearnt, the goal is to learn, without the knowledge of the original training dataset, a good predictor that is identical to the predictor that would have been produced when learning from scratch on the surviving examples.
            We propose a new ticketed model for learning--unlearning wherein the learning algorithm can send back additional information in the form of a small-sized (encrypted) ``ticket'' to each participating training example, in addition to retaining a small amount of ``central'' information for later. Subsequently, the examples that wish to be unlearnt present their tickets to the unlearning algorithm, which additionally uses the central information to return a new predictor. We provide space-efficient ticketed learning--unlearning schemes for a broad family of concept classes, including thresholds, parities, intersection-closed classes, among others.
            En route, we introduce the count-to-zero problem, where during unlearning, the goal is to simply know if there are any examples that survived. We give a ticketed learning--unlearning scheme for this problem that relies on the construction of Sperner families with certain properties, which might be of independent interest.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.12081" target="_blank">Computationally Efficient PAC RL in POMDPs with Latent Determinism and Conditional Embeddings</a> </strong> &nbsp; <a class="text-button3">ICML 2023</a>
        <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs14&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.12081">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs14">We study reinforcement learning with function approximation for large-scale Partially Observable Markov Decision Processes (POMDPs) where the state space and observation space are large or even continuous. Particularly, we consider Hilbert space embeddings of POMDP where the feature of latent states and the feature of observations admit a conditional Hilbert space embedding of the observation emission process, and the latent state transition is deterministic. Under the function approximation setup where the optimal latent state-action Q-function is linear in the state feature, and the optimal Q-function has a gap in actions, we provide a \emph{computationally and statistically efficient} algorithm for finding the \emph{exact optimal} policy. We show our algorithm's computational and statistical complexities scale polynomially with respect to the horizon and the intrinsic dimension of the feature on the observation space. Furthermore, we show both the deterministic latent transitions and gap assumptions are necessary to avoid statistical complexity exponential in horizon or dimension. Since our guarantee does not have an explicit dependence on the size of the state and observation spaces, our algorithm provably scales to large-scale POMDPs.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2210.06718" target="_blank">Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</a> </strong> &nbsp; <a class="text-button3">ICLR 2023</a>
        <br> Yuda Song*, Yifei Zhou*, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs13&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2210.06718">ArXiv</a> | 
        <a href="https://github.com/yudasong/HyQ"><i class="fa fa-github w3-hover-opacity"></i></a> 

        </p><p align="justify"> <i style="display: none;" id="dash_abs13">We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outperforms state-of-the-art online, offline, and hybrid RL baselines on challenging benchmarks, including Montezuma's Revenge.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2210.06705" target="_blank">From Gradient Flow on Population Loss to Learning with Stochastic Gradient Descent</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>
        <br> Satyen Kale, Jason D. Lee, Chris De Sa, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan. 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs12&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2210.06705">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs12">Stochastic Gradient Descent (SGD) has been the method of choice for learning large-scale non-convex models. While a general analysis of when SGD works has been elusive, there has been a lot of recent progress in understanding the convergence of Gradient Flow (GF) on the population loss, partly due to the simplicity that a continuous-time analysis buys us. An overarching theme of our paper is providing general conditions under which SGD converges, assuming that GF on the population loss converges. Our main tool to establish this connection is a general converse Lyapunov like theorem, which implies the existence of a Lyapunov potential under mild assumptions on the rates of convergence of GF. In fact, using these potentials, we show a one-to-one correspondence between rates of convergence of GF and geometrical properties of the underlying objective. When these potentials further satisfy certain self-bounding properties, we show that they can be used to provide a convergence guarantee for Gradient Descent (GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It turns out that these self-bounding assumptions are in a sense also necessary for GD/SGD to work. Using our framework, we provide a unified analysis for GD/SGD not only for classical settings like convex losses, or objectives that satisfy PL / KL properties, but also for more complex problems including Phase Retrieval and Matrix sq-root, and extending the results in the recent work of Chatterjee 2022.
       </i></p>  

    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.13063" target="_blank">On the Complexity of Adversarial Decision Making</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>          <a class="text-button">Oral</a>

        <br> Dylan J. Foster, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <!-- <br> <font color="red">(Oral Presentation)</font> --> 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs11&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.13063">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs11">A central problem in online learning and decision making -- from bandits to reinforcement learning -- is to understand what modeling assumptions lead to sample-efficient learning guarantees. We consider a general adversarial decision making framework that encompasses (structured) bandit problems with adversarial rewards and reinforcement learning problems with adversarial dynamics. Our main result is to show -- via new upper and lower bounds -- that the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. in the stochastic counterpart to our setting, is necessary and sufficient to obtain low regret for adversarial decision making. However, compared to the stochastic setting, one must apply the Decision-Estimation Coefficient to the convex hull of the class of models (or, hypotheses) under consideration. This establishes that the price of accommodating adversarial rewards or dynamics is governed by the behavior of the model class under convexification, and recovers a number of existing results -- both positive and negative. En route to obtaining these guarantees, we provide new structural results that connect the Decision-Estimation Coefficient to variants of other well-known complexity measures, including the Information Ratio of Russo and Van Roy and the Exploration-by-Optimization objective of Lattimore and GyÃ¶rgy.
       </i></p>  

    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.12020" target="_blank">Provably Efficient Reinforcement Learning in Partially Observable Dynamical Systems</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>
        <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
        <!--<br> <strong>NeurIPS 2022.</strong> --> 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs10&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.12020">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs10">We study Reinforcement Learning for partially observable dynamical systems using function approximation. We propose a new \textit{Partially Observable Bilinear Actor-Critic framework}, that is general enough to include models such as observable tabular Partially Observable Markov Decision Processes (POMDPs), observable Linear-Quadratic-Gaussian (LQG), Predictive State Representations (PSRs), as well as a newly introduced model Hilbert Space Embeddings of POMDPs and observable POMDPs with latent low-rank transition. Under this framework, we propose an actor-critic style algorithm that is capable of performing agnostic policy learning. Given a policy class that consists of memory based policies (that look at a fixed-length window of recent observations), and a value function class that consists of functions taking both memory and future observations as inputs, our algorithm learns to compete against the best memory-based policy in the given policy class. For certain examples such as undercomplete observable tabular POMDPs, observable LQGs and observable POMDPs with latent low-rank transition, by implicitly leveraging their special properties, our algorithm is even capable of competing against the globally optimal policy without paying an exponential dependence on the horizon in its sample complexity.
       </i></p>  
    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.09421" target="_blank">Guarantees for Epsilon-Greedy Reinforcement Learning with Function Approximation</a> </strong> &nbsp; <a class="text-button3">ICML 2022</a>
        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href="https://rldm.org/">RLDM 2022</a> - Reinforcement Learning and Decision Making conference.
        <br>               <a href="javascript:toggleblock(&#39;dash_abs9&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.09421">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs9">Myopic exploration policies such as epsilon-greedy, softmax, or Gaussian noise fail to explore efficiently in some reinforcement learning tasks and yet, they perform well in many others. In fact, in practice, they are often selected as the top choices, due to their simplicity. But, for what tasks do such policies succeed? Can we give theoretical guarantees for their favorable performance? These crucial questions have been scarcely investigated, despite the prominent practical importance of these policies. This paper presents a theoretical analysis of such policies and provides the first regret and sample-complexity bounds for reinforcement learning with myopic exploration. Our results apply to value-function-based algorithms in episodic MDPs with bounded Bellman Eluder dimension. We propose a new complexity measure called myopic exploration gap, denoted by alpha, that captures a structural property of the MDP, the exploration policy and the given value function class. We show that the sample-complexity of myopic exploration scales quadratically with the inverse of this quantity, 1 / alpha^2. We further demonstrate through concrete examples that myopic exploration gap is indeed favorable in several tasks where myopic exploration succeeds, due to the corresponding dynamics and reward structure.
       </i></p>  
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2107.05074" target="_blank">SGD: The Role of Implicit Regularization, Batch-size and Multiple Epochs</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Satyen Kale, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs8&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2107.05074">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs8">Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the method of choice for learning with large over-parameterized models. A popular theory for explaining why SGD works well in practice is that the algorithm has an implicit regularization that biases its output towards a good solution. Perhaps the theoretically most well understood learning setting for SGD is that of Stochastic Convex Optimization (SCO), where it is well known that SGD learns at a rate of O(1/nâ€¾âˆš), where n is the number of samples. In this paper, we consider the problem of SCO and explore the role of implicit regularization, batch size and multiple epochs for SGD. Our main contributions are threefold:
            (a) We show that for any regularizer, there is an SCO problem for which Regularized Empirical Risk Minimzation fails to learn. This automatically rules out any implicit regularization based explanation for the success of SGD.
            (b) We provide a separation between SGD and learning via Gradient Descent on empirical loss (GD) in terms of sample complexity. We show that there is an SCO problem such that GD with any step size and number of iterations can only learn at a suboptimal rate: at least Î©Ëœ(1/n5/12).
            (c) We present a multi-epoch variant of SGD commonly used in practice. We prove that this algorithm is at least as good as single pass SGD in the worst case. However, for certain SCO problems, taking multiple passes over the dataset can significantly outperform single pass SGD.
            We extend our results to the general learning setting by showing a problem which is learnable for any data distribution, and for this problem, SGD is strictly better than RERM for any regularization function. We conclude by discussing the implications of our results for deep learning, and show a separation between SGD and ERM for two layer diagonal neural networks.     
        </i></p>  
    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2106.11519" target="_blank">Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>          <a class="text-button">Spotlight</a>

        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs7&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2106.11519">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs7">There have been many recent advances on provably efficient Reinforcement Learning (RL) in problems with rich observation spaces. However, all these works share a strong realizability assumption about the optimal value function of the true MDP. Such realizability assumptions are often too strong to hold in practice. In this work, we consider the more realistic setting of agnostic RL with rich observation spaces and a fixed class of policies Î  that may not contain any near-optimal policy. We provide an algorithm for this setting whose error is bounded in terms of the rank d of the underlying MDP. Specifically, our algorithm enjoys a sample complexity bound of O((H4dK3dlog|Î |)/Ïµ2) where H is the length of episodes, K is the number of actions and Ïµ&gt;0 is the desired sub-optimality. We also provide a nearly matching lower bound for this agnostic setting that shows that the exponential dependence on rank is unavoidable, without further assumptions.
       </i></p>  
        <!--<br> <font color="red">(Spotlight Presentation)</font>-->
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2103.03279" target="_blank">Remember What You Want to Forget: Algorithms for Machine Unlearning</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh 
        <br> Short version at <a href="https://tpdp.journalprivacyconfidentiality.org/2021/">TPDP 2021</a> - Theory and Practice of Differential Privacy.
        <br>               <a href="javascript:toggleblock(&#39;dash_abs6&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2103.03279">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs6">We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset S drawn i.i.d. from an unknown distribution, and outputs a model wË† that performs well on unseen samples from the same distribution. However, at some point in the future, any training datapoint zâˆˆS can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees. We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity.
            For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to O(n/d1/4) samples, where d is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of O(n/d1/2) samples. This demonstrates a novel separation between differential privacy and machine unlearning. 
              </i></p>  
    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2106.03243" target="_blank">Neural Active Learning with Performance Guarantees</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Zhilei Wang, Pranjal Awasthi, Christoph Dann, Ayush Sekhari, and Claudio Gentile 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs5&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2106.03243">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs5">We investigate the problem of active learning in the streaming setting in non-parametric regimes, where the labels are stochastically generated from a class of functions on which we make no assumptions whatsoever. We rely on recently proposed Neural Tangent Kernel (NTK) approximation tools to construct a suitable neural embedding that determines the feature space the algorithm operates on and the learned model computed atop. Since the shape of the label requesting threshold is tightly related to the complexity of the function to be learned, which is a-priori unknown, we also derive a version of the algorithm which is agnostic to any prior knowledge. This algorithm relies on a regret balancing scheme to solve the resulting online model selection problem, and is computationally efficient. We prove joint guarantees on the cumulative regret and number of requested labels which depend on the complexity of the labeling function at hand. In the linear case, these guarantees recover known minimax results of the generalization error as a function of the label complexity in a standard statistical learning setting.
       </i></p>  
    <p></p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2005.03789" target="_blank">Reinforcement Learning with Feedback Graphs</a></strong> &nbsp; <a class="text-button3">NeurIPS 2020</a>
        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href="https://sites.google.com/view/icml2018nonconvex/">ICML 2020 Theoretical Foundations of RL workshop.</a> 
        <br>               <a href="javascript:toggleblock(&#39;dash_abs4&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2005.03789"> ArXiv</a> 

        </p><p align="justify"> <i style="display: none;" id="dash_abs4">We study episodic reinforcement learning in Markov decision processes when the agent receives additional feedback per step in the form of several transition observations. Such additional observations are available in a range of tasks through extended sensors or prior knowledge about the environment (e.g., when certain actions yield similar outcome). We formalize this setting using a feedback graph over state-action pairs and show that model-based algorithms can leverage the additional feedback for more sample-efficient learning. We give a regret bound that, ignoring logarithmic factors and lower-order terms, depends only on the size of the maximum acyclic subgraph of the feedback graph, in contrast with a polynomial dependency on the number of states and actions in the absence of a feedback graph. Finally, we highlight challenges when leveraging a small dominating set of the feedback graph as compared to the bandit setting and propose a new algorithm that can use knowledge of such a dominating set for more sample-efficient learning of a near-optimal policy.
       </i></p>  
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2006.13476" target="_blank">Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations</a></strong> &nbsp; <a class="text-button3">COLT 2020</a>
        <br> Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Honorable mention for best talk award at <a href="https://www.nyas.org/events/2020/14th-annual-machine-learning-symposium/">NYAS ML symposium 2020</a>.
        <br>               <a href="javascript:toggleblock(&#39;dash_abs3&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/2006.13476">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs3">We design an algorithm which finds an Ïµ-approximate stationary point (with â€–âˆ‡F(x)â€–â‰¤Ïµ) using O(Ïµâˆ’3) stochastic gradient and Hessian-vector products, matching guarantees that were previously available only under a stronger assumption of access to multiple queries with the same random seed. We prove a lower bound which establishes that this rate is optimal and---surprisingly---that it cannot be improved using stochastic pth order methods for any pâ‰¥2, even when the first p derivatives of the objective are Lipschitz. Together, these results characterize the complexity of non-convex stochastic optimization with second-order methods and beyond. Expanding our scope to the oracle complexity of finding (Ïµ,Î³)-approximate second-order stationary points, we establish nearly matching upper and lower bounds for stochastic second-order methods. Our lower bounds here are novel even in the noiseless case.

       </i></p>  
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/1902.04686" target="_blank">The Complexity of Making the Gradient Small in Stochastic Convex Optimization</a></strong> &nbsp; <a class="text-button3">COLT 2019</a>           <a class="text-button">Best Student Paper Award</a>
 
        <br> Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, Ohad Shamir, Nathan Srebro, Karthik Sridharan, Blake Woodworth  
        <br>               <a href="javascript:toggleblock(&#39;dash_abs2&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/1902.04686">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs2">We give nearly matching upper and lower bounds on the oracle complexity of finding Ïµ-stationary points (â€–âˆ‡F(x)â€–â‰¤Ïµ) in stochastic convex optimization. We jointly analyze the oracle complexity in both the local stochastic oracle model and the global oracle (or, statistical learning) model. This allows us to decompose the complexity of finding near-stationary points into optimization complexity and sample complexity, and reveals some surprising differences between the complexity of stochastic optimization versus learning. Notably, we show that in the global oracle/statistical learning model, only logarithmic dependence on smoothness is required to find a near-stationary point, whereas polynomial dependence on smoothness is necessary in the local stochastic oracle model. In other words, the separation in complexity between the two models can be exponential, and that the folklore understanding that smoothness is required to find stationary points is only weakly true for statistical learning.
            Our upper bounds are based on extensions of a recent "recursive regularization" technique proposed by Allen-Zhu (2018). We show how to extend the technique to achieve near-optimal rates, and in particular show how to leverage the extra information available in the global oracle model. Our algorithm for the global model can be implemented efficiently through finite sum methods, and suggests an interesting new computational-statistical tradeoff. 
              </i></p>  
        <!-- <br> <strong> <font color="red">(Best Student Paper Award)</font> </strong>--> 
    <p></p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/1810.11059" target="_blank">Uniform Convergence of Gradients for Non-Convex Learning and Optimization</a></strong> &nbsp; <a class="text-button3">NeurIPS 2018</a>
        <br> Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href="https://sites.google.com/view/icml2018nonconvex/" target="_blank">ICML 2018 Nonconvex Optimization workshop.</a>
        <br>               <a href="javascript:toggleblock(&#39;dash_abs1&#39;)">Abstract</a> |
        <a href="https://arxiv.org/abs/1810.11059">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        </p><p align="justify"> <i style="display: none;" id="dash_abs1">We investigate 1) the rate at which refined properties of the empirical risk---in particular, gradients---converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed.
            Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.
       </i></p>  
    <p></p>
</li>


<div style="height: 20px;"></div>

<br> <br> <br> <br> 
</ul></div>
              
              <script>
                // Fetch the content from publications.html and insert it into the div
                fetch('publications.html')
                  .then(response => {
                    if (!response.ok) {
                      throw new Error('Network response was not ok');
                    }
                    return response.text();
                  })
                  .then(data => {
                    document.getElementById('publications-container').innerHTML = data;
                  })
                  .catch(error => console.error('Error loading the file:', error));
              </script>        
            </div></div><div id="mCSB_2_scrollbar_vertical" class="mCSB_scrollTools mCSB_2_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: block;"><div class="mCSB_draggerContainer"><div id="mCSB_2_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; display: block; height: 202px; max-height: 867px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

          <!-- Publications subpage -->
            <section class="pt-page pt-page-4 mCustomScrollbar _mCS_3" data-id="InvitedTalks"><div id="mCSB_3" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_3_container" class="mCSB_container" style="position:relative; top:0; left:0;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Invited Talks</h2>
                    <!-- <h5 class="section-description">Advise?</h5> -->
                  </div>
                </div>
                  
              <div class="block-title">
                <h2>Invited Talks</h2> 
              </div>

              <div id="InvitedTalks-container"><ul style="margin-left: 2.5em;">
 <li>
    <b>Machine Unlearning: Where does the practice stand?</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li style="margin: 0; padding: 0;">Princeton Language and Intelligence seminar, Princeton University, USA <span style="float: right;">Dec 2024</span></li>
    </ol>
  </li>
  <br> 
  <li>
    <b>Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li style="margin: 0; padding: 0;">COLT, Edmonton, CA <span style="float: right;">Jun 2024</span></li>
      <li style="margin: 0; padding: 0;">Adaptive Learning in Complex Environments Workshop, TTIC, Chicago, USA <span style="float: right;">Apr 2024</span></li>
    </ol>
  </li>
  <br>
  <li><b>Offline Data Enhanced On-Policy Policy Gradient</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Virtual RL Theory Seminar Series <span style="float: right;">Apr 2024</span></li>
      <li>CSA Theory Seminar, IISc Bangalore, India <span style="float: right;">Apr 2024</span></li>
    </ol>
  </li>
  <br>

  <li><b>Ticketed Learning-Unlearning Schemes</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Rutgers University, Piscataway, USA <span style="float: right;">Dec 2024</span></li>
      <li>Max Planck Institute for Intelligent Systems, Tubingen, Germany <span style="float: right;">May 2024</span></li>
      <li>Conference on Learning Theory (COLT), Bangalore, India <span style="float: right;">Jul 2023</span></li>
      <li>CS Theory Seminar, Cornell University, Ithaca, USA <span style="float: right;">Nov 2023</span></li>
      <li>Annual Conference on Information Sciences and Systems (CISS), Princeton, USA <span style="float: right;">Mar 2024</span></li>
      <li>CS Theory Seminar, University of Pennsylvania, Philadelphia, USA <span style="float: right;">Mar 2024</span></li>
      <li>CSA Theory Seminar, IISc Bangalore, India <span style="float: right;">Apr 2024</span></li>
    </ol>
  </li>
  <br>
  <li><b>Machine Unlearning: Algorithms, complexity, and new challenges</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Meta AI Research, USA <span style="float: right;">Apr 2023</span></li>
    </ol>
  </li>
  <br> 
  <li><b>Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>ImprobableAI (Prof. Pulkit Agarwal's lab) meeting, MIT <span style="float: right;">Mar 2023</span></li>
    </ol>
  </li>
  <br> 
  <li><b>On the Complexity of Adversarial Decision Making</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Virtual RL Theory Seminar Series <span style="float: right;">Jul 2023</span></li>
      <li>BLISS Seminar, UC Berkeley, USA <span style="float: right;">Feb 2023</span></li>
      <li>Information Theory and Applications (ITA) Workshop, San Diego, USA <span style="float: right;">Feb 2023</span></li>
      <li>Theory Seminar, UCSD, San Diego, USA <span style="float: right;">Feb 2023</span></li>
      <li>Microsoft Research NYC, USA <span style="float: right;">Feb 2023</span></li>
      <li>ML Tea, Massachusetts Institute of Technology, USA <span style="float: right;">Apr 2023</span></li>
    </ol>
  </li>
  <br>
  <li><b>When does SGD learn?</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Prof. Dan Roy's Lab, University of Toronto, CA <span style="float: right;">Oct 2022</span></li>
      <li>Mathematical Foundations of Deep Learning Reading Group, ETH Zurich <span style="float: right;">Nov 2022</span></li>
    </ol>
  </li>
  <br>
  <li><b>Remember What You Want to Forget: Algorithms for Machine Unlearning</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>AI Seminar, Cornell University <span style="float: right;">Feb 2022</span></li>
      <li>Prof. Jiantao Jiao's Lab, UC Berkeley <span style="float: right;">Aug 2021</span></li>
    </ol>
  </li>
  <br>
  <li><b>SGD: The Role of Implicit Regularization, Batch-Size and Multiple Epochs</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Mathematical Foundations of Deep Learning Reading Group, ELLIS, ETH Zurich <span style="float: right;">May 2022</span></li>
      <li>Foundations of Data Science (FODS) Seminar, IISC (Tsinghua University), China <span style="float: right;">Apr 2022</span></li>
      <li>Collaboration on the Theoretical Foundations of Deep Learning (MODL) Monthly Meeting <span style="float: right;">Feb 2022</span></li>
      <li>Theory Seminar, Cornell University <span style="float: right;">May 2021</span></li>
      <li>Algorithms and Theory Seminar, University of Waterloo (CA) <span style="float: right;">Nov 2021</span></li>
      <li>Learning Theory Seminar, Google Research NY <span style="float: right;">Nov 2021</span></li>
    </ol>
  </li>
  <br>
  <li><b>Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Artificial Intelligence (AI) Seminar, Cornell University <span style="float: right;">Mar 2021</span></li>
      <li>RL Reading Group, Cornell University <span style="float: right;">Jun 2021</span></li>
    </ol>
  </li>
  <br>
  <li><b>Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Highlights Beyond SIGMETRICS 2021, Beijing, China (Virtual) <span style="float: right;">Jun 2021</span></li>
      <li>Spotlight Talk, Annual ML Symposium, New York Academy of Sciences (NYAS) <span style="float: right;">Mar 2020</span></li>
      <li style="padding-left: 0.7em; color: red;">Best Talk Award, Honorable Mention</li>
      <li>Conference on Learning Theory (COLT), Conference Talk <span style="float: right;">Jul 2020</span></li>
      <li>Learning Theory Seminar, Google NYC <span style="float: right;">Nov 2020</span></li>
      <li>Theory Tea, Cornell University <span style="float: right;">Nov 2020</span></li>
    </ol>
  </li>
  <br>
  <li><b>The Complexity of Making the Gradient Small in Stochastic Convex Optimization</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li>Intern Talk Series, Google Research, New York <span style="float: right;">Jul 2019</span></li>
      <li>Theory Seminar, Cornell University <span style="float: right;">Nov 2019</span></li>
    </ol>
  </li>
  <br>
  <li><b>Uniform Convergence of Gradients for Non-Convex Learning and Optimization</b>
    <ol style="list-style-type: none; padding-top: 3pt; margin: 0; padding: 0;">
      <li> Workshop on Modern Trends in Non-Convex Optimization for ML, ICML, <span style="float: right;">Jun 2018</span></li>
      <li>Annual ML Symposium, New York Academy of Sciences (NYAS) <span style="float: right;">Feb 2019</span></li>
    </ol>
  </li>
  <br>
</ul>
</div>
              
              <script>
                // Fetch the content from publications.html and insert it into the div
                fetch('talks.html') 
                  .then(response => {
                    if (!response.ok) {
                      throw new Error('Network response was not ok');
                    }
                    return response.text();
                  })
                  .then(data => {
                    document.getElementById('InvitedTalks-container').innerHTML = data;
                  })
                  .catch(error => console.error('Error loading the file:', error));
              </script>        
            </div></div><div id="mCSB_3_scrollbar_vertical" class="mCSB_scrollTools mCSB_3_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: block;"><div class="mCSB_draggerContainer"><div id="mCSB_3_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; display: block; height: 376px; max-height: 867px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

            <!-- /Portfolio Subpage -->
<!--             <section class="pt-page pt-page-4" data-id="InvitedTalks">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Invited Talks</h2>
                  </div>
                </div>
                  
                  <div class="block-title">
                  <div>
                    <h2>Teaching Assistant at Cornell University</h2> 
                    <ul> 
                    <li> <p> <a href='http://www.cs.cornell.edu/courses/cs6783/2018fa/' target="_blank">Machine Learning Theory</a> - Fall 2018 (Graduate Level Course)
                        <br> Under Prof. Karthik Sridharan, Cornell University

                    </li>
                    
                    <li> <p> <a href='http://www.cs.cornell.edu/courses/cs4820/2018sp/' target="_blank">Introduction to Analysis of Algorithms</a> - Spring 2018
                        <br> Under Prof. Robert Kleinberg, Cornell University
                    </li>

                    <li> <p> <a href='http://www.cs.cornell.edu/courses/cs4786/2017fa/' target="_blank">Machine Learning for Data Science</a> - Fall 2017
                        <br> Under Prof. Karthik Sridharan, Cornell University 
                    </li>           
                    </ul>

                    </div>
            </section> -->

            <!-- Grad School Subpage -->
            <section class="pt-page pt-page-4 mCustomScrollbar _mCS_4 mCS_no_scrollbar" data-id="Teaching"><div id="mCSB_4" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_4_container" class="mCSB_container mCS_y_hidden mCS_no_scrollbar_y" style="position:relative; top:0; left:0;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Teaching</h2>
                    <!-- <h5 class="section-description">I love teaching!</h5> -->
                  </div>
                </div>
                  
                  <div class="block-title">
                  <div>
                    <h2>Teaching Assistant at Cornell University</h2> 
                    <ul> 
                    <li> <p> <a href="http://www.cs.cornell.edu/courses/cs6783/2018fa/" target="_blank">Machine Learning Theory</a> - Fall 2018 (Graduate Level Course)
                        <br> Under Prof. Karthik Sridharan, Cornell University

                    </p></li>
                    
                    <li> <p> <a href="http://www.cs.cornell.edu/courses/cs4820/2018sp/" target="_blank">Introduction to Analysis of Algorithms</a> - Spring 2018
                        <br> Under Prof. Robert Kleinberg, Cornell University
                    </p></li>

                    <li> <p> <a href="http://www.cs.cornell.edu/courses/cs4786/2017fa/" target="_blank">Machine Learning for Data Science</a> - Fall 2017
                        <br> Under Prof. Karthik Sridharan, Cornell University 
                    </p></li>           
                    </ul>

                    <h2>Teaching Assistant at IIT Kanpur</h2> 
                    <ul> 
                    <li> <p> <a href="https://palash97.github.io/C_lecture_notes/Intro.pdf" target="_blank">Fundamentals of Computing</a> - Fall 2015 
                        <br> Under Prof. Nitin Saxena, IIT Kanpur

                    </p></li>
                    
                    </ul>


                    </div>

            </div></div></div><div id="mCSB_4_scrollbar_vertical" class="mCSB_scrollTools mCSB_4_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: none;"><div class="mCSB_draggerContainer"><div id="mCSB_4_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; height: 0px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

                        <!-- Misc Subpage -->
                        <section class="pt-page pt-page-4 mCustomScrollbar _mCS_5" data-id="Misc"><div id="mCSB_5" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_5_container" class="mCSB_container" style="position:relative; top:0; left:0;" dir="ltr">
                          <div class="border-block-top-110"></div>
                          <div class="section-inner">
                            <div class="section-title-block">
                              <div class="section-title-wrapper">
                                <h2 class="section-title">Misc</h2>
                                <!-- <h5 class="section-description">I love teaching!</h5> -->
                              </div>
                            </div>
                              
                            <div class="block-title" ,="" style="margin-top: 30px;">
                              <h3>How to reach me?</h3>
                            </div>
                            <p>
                              Please feel free to reach out if you think that we have any shared interests. The best way to reach me is via email at <strong>sekhari [at] mit [dot] edu</strong>. 
                            </p>

                            <div class="block-title" ,="" style="margin-top: 30px;">
                              <h3>Useful Links</h3>
                            </div>
                            <ul, style="margin-top: 10px;">
                              <li>  
                                <a href="https://www.youtube.com/watch?v=a1zDuOPkMSw&amp;ab_channel=securitylectures">"You and Your Research"</a>, Richard Hamming (June 6, 1995). 
                              </li> 
                              <li>  
                                <a href="https://www.youtube.com/watch?v=Rn1w4MRHIhc&amp;ab_channel=TalksatGoogle">"How to Have a Bad Career
                                  in  Research/Academia"</a>, a talk by David Patterson. 
                              </li>
                              <li>  
                                <a href="https://www.youtube.com/watch?v=ji5_MqicxSo&amp;ab_channel=CarnegieMellonUniversity">"Last Lecture: Achieving Your Childhood Dreams"</a>, a talk by Randy Pausch at CMU. 
                              </li>
                              <li>  
                                <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/HumorLinks.htm">Witty Quotes</a> by J. Michael Steele. 
                              </li>

                              <li>
                                <a href="https://github.com/google-research/arxiv-latex-cleaner">Arxiv Latex Cleaner</a>.
                              </li>


                            

                            <div class="block-title" ,="" style="margin-top: 30px;">
                              <h3>Personal Interests</h3>
                              </div>
            
                          <p>
                            Outside my research activies, I find joy in <a href="https://ayush.sekhari.com/images/climbing.jpg" target="_blank">indoor climbing</a>, experimenting with <a href="https://www.instagram.com/my.vegetales/" target="_blank">plant-based cooking</a>, and consuming an absurd amount of <a href="https://www.tazachocolate.com/" target="_blank">chocolate</a>.
                          </p>

                        
                          <div class="block-title" ,="" style="margin-top: 30px;">
                            <h3>A Fascinating Art</h3> 
                          </div> 
                          
                        <p style="text-align:justify"> 
                        In 1989, <a href="https://en.wikipedia.org/wiki/Keith_Haring">Keith Haring</a> created a print series titled <a href="https://haringkids.com/art/the-story-of-red-and-blue" target="_blank">The Story of Red and Blue</a>, where he depicted characters inspired by both well-known children's stories and more mysterious sources. Throughout the series, the red and blue hues alternate, eventually merging within a human hand, symbolizing unity and conveying a message of hope and optimism. I find this artwork incredibly captivating and hope you enjoy it as well.
                        </p>

                        <br> 


                        
                        <img src="./Ayush Sekhari_files/image_in_bottom.jpg" class="mCS_img_loaded"> 

                        </ul,></div></div><div id="mCSB_5_scrollbar_vertical" class="mCSB_scrollTools mCSB_5_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: block;"><div class="mCSB_draggerContainer"><div id="mCSB_5_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; display: block; height: 745px; max-height: 867px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

            <!-- Portfolio Subpage -->
            <section class="pt-page pt-page-4 mCustomScrollbar _mCS_6 mCS_no_scrollbar" data-id="Service"><div id="mCSB_6" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_6_container" class="mCSB_container mCS_y_hidden mCS_no_scrollbar_y" style="position:relative; top:0; left:0;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Professional Service</h2> 
                    <!-- <h5 class="section-description">...</h5>-->
                  </div>
                </div>
                  
                      <h3>Learning Theory Alliance</h3> 

    <div>
      I am a part of the <a href="https://www.let-all.com/index.html" target="_blank"> Learning Theory Alliance (LetAll) </a> organizing committee. <br><br> 

      <p style="margin-left: 50px; margin-right: 40px; text-align: justify;">
      <font> <em> Our mission is to develop a strong, supportive learning theory community and ensure its healthy growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers</em></font>. 
    </p>
      Please feel free to <a href="mailto:learning.theory.alliance@gmail.com"> contact us </a> for any questions / feedback / suggesions regarding LetAll. We would love to hear from you.  

      <br><br> 

      <h3>Other Professional Activities</h3> 
      <ul style="list-style:none;">

      <li><strong>Organization</strong> 
        <ul> 
      <li> Organized workshop on <a href="https://upml2022.github.io/" target="_blank">Updatable Machine Learning</a> at ICML 2022 (with <a href="https://people.ece.cornell.edu/acharya/" target="_blank">Prof. Jayadev Acharya</a> and <a href="http://www.gautamkamath.com/" target="_blank">Prof. Gautam Kamath</a>)</li>
      </ul>
        </li>

      <li><strong>Volunteer</strong>  
        <ul>
      <li> Poster sessions committee at <a href="https://www.widscambridge.org/" target="_blank"> Women in Data Science, Cambridge</a>  2024.</li> 
      <li> Communications Committee at <a href="https://www.let-all.com/index.html" target="_blank">  Learning Theory Alliance </a> (2023-current).</li>          
      <li> Organization of Mentoring Tables at <a href="https://www.let-all.com/fall23.html" target="_blank"> Fall 2023 Mentoriship Workshop</a>  under <a href="https://www.let-all.com/index.html" target="_blank">  Learning Theory Alliance</a>.</li> 
      </ul>
        </li>


      </ul>

      <h3>PC Member</h3>
      <ul style="list-style:none;">
        <ul>
      <li>Algorithmic Learning Theory (ALT) 2024 
      </li> 
      <li>
        Conference on Learning Theory (COLT) 2019-22 (Junior PC Member) 
      </li>
      </ul>
        
      </ul> 


            <h3>Reviewing</h3>
      <ul style="list-style:none;">
        <ul>
      <li><strong>Journals:</strong> Journal of Complexity 2021, JMLR (2020-22)
      </li> 
      <li><strong>Conferences:</strong> NeurIPS (2019-22), ALT (2020-22), ICLR 2019, AISTATS 2019, ICML (2019-21), ISIT 2020, ITCS 2020, FORC 2021
      <!--<li><strong>Workshops</strong> 
      </li>--> 
      </li></ul>
        
      </ul> 

      </div>

  </div></div><div id="mCSB_6_scrollbar_vertical" class="mCSB_scrollTools mCSB_6_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: none;"><div class="mCSB_draggerContainer"><div id="mCSB_6_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; height: 0px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section>

            <!-- Contact Subpage -->
            <section class="pt-page pt-page-4 mCustomScrollbar _mCS_7 mCS_no_scrollbar" data-id="contact"><div id="mCSB_7" class="mCustomScrollBox mCS-light mCSB_vertical mCSB_inside" style="max-height: none;" tabindex="0"><div id="mCSB_7_container" class="mCSB_container mCS_y_hidden mCS_no_scrollbar_y" style="position:relative; top:0; left:0;" dir="ltr">
              <div class="border-block-top-110"></div>
              <div class="section-inner">
                <div class="section-title-block">
                  <div class="section-title-wrapper">
                    <h2 class="section-title">Contact</h2>
                    <!-- <h5 class="section-description">Get in Touch</h5> -->
                  </div>
                </div>

                <div class="row">
                  <div class="col-sm-6 col-md-6 subpage-block">
<!--                     <div class="block-title">
                      <h3>Letâ€™s do something amazing.</h3>
                    </div>
 -->                
                    <div class="justify">
                    <p> The best way to reach me is via email. </p>
                      </div>
                    <div class="contact-info-block">
                      <div class="ci-icon">
                        <i class="pe-7s-icon pe-7s-mail"></i>
                      </div>
                      <div class="ci-text">
                        <h5>sekhari [at] mit [dot] edu</h5>
                      </div>
                    </div>
                    <div class="contact-info-block">
                      <div class="ci-icon">
                        <i class="pe-7s-icon pe-7s-map-marker"></i>
                      </div>
                      <div class="ci-text">
                        <h5><a href="https://www.google.com/maps/place/MIT+Institute+for+Data,+Systems,+and+Society+(IDSS)/@42.361976,-71.087936,15z/data=!4m2!3m1!1s0x0:0x8082c30c6bb4140b?sa=X&amp;ved=1t:2428&amp;ictx=111&amp;cshid=1729372147043563" target="_blank">
                          Institute for Data, Systems, and Society (IDSS), MIT</a><a></a></h5><a>
                      </a></div><a>
                    </a></div><a>
                  </a></div><a>

                  <div class="col-sm-6 col-md-6 subpage-block">

                      <div class="messages"></div>
                      <img src="./Ayush Sekhari_files/Hello.png" alt="image" class="mCS_img_loaded">

                        <!-- <input type="submit" class="button btn-send" value="Send message"> -->
                      </div>
                    
                  </a></div><a>
                </a></div><a>
              </a></div><div id="mCSB_7_scrollbar_vertical" class="mCSB_scrollTools mCSB_7_scrollbar mCS-light mCSB_scrollTools_vertical" style="display: none;"><div class="mCSB_draggerContainer"><div id="mCSB_7_dragger_vertical" class="mCSB_dragger" style="position: absolute; min-height: 30px; height: 0px; top: 0px;"><div class="mCSB_dragger_bar" style="line-height: 30px;"></div></div><div class="mCSB_draggerRail"></div></div></div></div></section><div id="page-ajax-loaded" class="page-ajax-loaded animated rotateInDownRight"></div></div><a>
            
            <!-- End Contact Subpage -->

          </a></div><a>
        </a></div><a>
        <!-- /Page changer wrapper -->
      </a></div><a>
      <!-- /Main Content -->
    

    <!-- <script data-cfasync="false" src="../../../cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="js/bootstrap.min.js"></script> -->
    <script src="./Ayush Sekhari_files/pages-switcher.js"></script>
    <script src="./Ayush Sekhari_files/imagesloaded.pkgd.min.js"></script>
    <script src="./Ayush Sekhari_files/validator.js"></script>
    <script src="./Ayush Sekhari_files/jquery.shuffle.min.js"></script>
    <script src="./Ayush Sekhari_files/masonry.pkgd.min.js"></script> 
    <script src="./Ayush Sekhari_files/owl.carousel.min.js"></script>
    <script src="./Ayush Sekhari_files/jquery.magnific-popup.min.js"></script>
    <script src="./Ayush Sekhari_files/jquery.mCustomScrollbar.concat.min.js"></script>
    <script src="./Ayush Sekhari_files/tilt.jquery.min.js"></script>
    <script src="./Ayush Sekhari_files/jquery.hoverdir.js"></script>
    <script src="./Ayush Sekhari_files/main.js"></script>

    <!-- Demo Color changer Script -->
    <!-- <script src="preview/lmpixels-demo-panel.js"></script> -->
    <!-- /Demo Color changer Script -->
  

</a></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>